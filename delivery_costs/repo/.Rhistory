setnames(., c('measure_new', 'measure_old'), c('measure_new_raw', 'measure_old_raw'))
#merge themes
theme_dt <- merge(ranks_old$theme[, .(geocode, item, theme,
index_old=theme_rank_integer)],
ranks_new$theme[, .(geocode, item, theme,
index_new=theme_rank_integer)],
by=c('geocode', 'item', 'theme')) %>%
.[, index_shift := index_new-index_old] %>%
.[, index_old_neg := index_old * -1] %>%
.[, index_shift_capped := index_shift] %>%
.[index_shift>=5, index_shift_capped := 5] %>%
.[index_shift<=-5, index_shift_capped := -5]
#merge indexes (old v. new) to compare
index_dt <- merge(ranks_old$index[, .(geocode, item, theme,
index_old=index_rank_integer)],
ranks_new$index[, .(geocode, item, theme,
index_new=index_rank_integer,
index_new_cal=index_rank_integer_cal)],
by=c('geocode', 'item', 'theme')) %>%
.[, scaling_effect := index_new/index_new_cal] %>%
.[, index_shift := index_new-index_old] %>%
.[, index_old_neg := index_old * -1] %>%
.[, index_shift_capped := index_shift] %>%
.[index_shift>=5, index_shift_capped := 5] %>%
.[index_shift<=-5, index_shift_capped := -5]
#identify dropouts
threshold_val <- 9
index_dt[, impacted_new := 0]
index_dt[, impacted_old := 0]
index_dt[index_new>=threshold_val, impacted_new := 1]
index_dt[index_old>=threshold_val, impacted_old := 1]
index_dt[, dropout := as.factor(impacted_old - impacted_new)]
index_dt[, GEOID10 := as.character(geocode)]
#merge the dropouts back onto the other tables for graphing
measure_ranks <- merge(measure_ranks, index_dt[, .(geocode, dropout)], by='geocode')
#drop water codes with weird data from maps
drop_geocodes <- c('53057990100' #san juan water area with lots of big changes
)
#add life expectancy data
index_dt[, geocode_chr := geocode %>% as.character]
index_dt <- merge(index_dt, le_dt, by.x='geocode_chr', by.y='geocode')
index_dt[, le_state_average := mean(le, na.rm=T)]
is_outlier <- function(x) {
return(x < quantile(x, 0.25, na.rm=T) - 1.5 * IQR(x, na.rm=T) | x > quantile(x, 0.75, na.rm=T) + 1.5 * IQR(x, na.rm=T))
}
#label the outliers
index_dt[, outlier_lab := ifelse(is_outlier(le), name, NA_character_), by=index_new]
#scaling effect basic scatterplot
ggplot(index_dt, aes(x=index_new, y=index_new_cal, color=scaling_effect) ) +
geom_point(position='jitter') +
#geom_hex(bins = 70) +
scale_color_viridis(option='magma') +
theme_bw()
# ----CONFIG------------------------------------------------------------------------------------------------------------
# clear memory
rm(list=ls())
install.packages('devtools')
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
remove.packages('cli')
install.packages("devtools")
install.packages("devtools")
sessionInfo()
sessionInfo()
# ----HEADER------------------------------------------------------------------------------------------------------------
# Author: JF
# Date: 01/18/2023
# Purpose: Prepare datasets and run delivery costs analysis
# source("/homes/jfrostad/_code/start/ipm/delivery_costs/calc.R", echo=T)
#***********************************************************************************************************************
# ----CONFIG------------------------------------------------------------------------------------------------------------
# clear memory
rm(list=ls())
#set opts
set.seed(98118)
options(scipen=999) #readability
#use cairo to render instead of quartz (quartz causes big slowdowns with geom_sf)
if(!identical(getOption("bitmapType"), "cairo") && isTRUE(capabilities()[["cairo"]])){
options(bitmapType = "cairo")
}
#set control flow params
reload <- T #set true if you want to reprep all the data
## Set core_repo locations
user            <- Sys.info()['user']
box_name        <- 'Joseph Frostad' #TODO find out how to pull this relative
local_dir       <- ifelse(Sys.info()["sysname"] == "Linux",
file.path('/homes', user, ''),
file.path('C:/Users', user, 'Documents/start/ipm/delivery_costs/'))
my_repo <- file.path(local_dir, 'repo')
my_dropbox <- file.path('C:/Users', user, 'UW START Dropbox', box_name, '/INTERNAL IPM Vx Valuations/NTD 2022')
setwd(my_repo)
#TODO categorize and comment packages
#TODO cleanup here
pacman::p_load(readxl, janitor, data.table, naniar, dplyr,
magrittr, scales, ggplot2, ggridges, ggrepel, gridExtra, RColorBrewer,
sf, viridis,
stargazer,
#caret, mlbench, randomForest, pls,
zoo)
#***********************************************************************************************************************
# ----IN/OUT------------------------------------------------------------------------------------------------------------
###Input###
code_dir <- file.path(my_repo, 'code')
#TODO consider which data is universal and which is by disease. How to organize
model_data_dir <- file.path(my_dropbox, '/1 - Data gathering - all diseases/Costs/Delivery costs/')
coef_path <- "Delivery_cost_regression_coefficients_to_use_7_12_2022.csv" #TODO rename
#ihme data
ihme_version <- '2022-09-13'
ihme_data_dir <- file.path(my_dropbox, 'IHME', ihme_version)
pop_path <- 'IU_pop_dens_all_age.csv'
#ntd mc data
mc_data_dir <- file.path(my_dropbox, 'LF/NTD MC')
#TODO for now we are just doing this for LF. in the future should be a function with disease type as an argument
lf_version <- '2022-08-26'
lf_data_dir <- file.path(my_dropbox, paste0('LF/InputData ', lf_version))
lf_path <- 'MDA_costs_22_08_2022_AM.csv'
# example_path <- "Example data/Example_data_for_testing_code_7_12_2022.csv"
# cost_path <- 'Cost_data_23_08_2022_complete_to_share.csv'
###Output###
lf_out_dir <- file.path(my_dropbox, paste0('LF/OutputData ', lf_version))
out_dir <- file.path(local_dir, 'output')
viz_dir  <- file.path(local_dir, 'viz')
#***********************************************************************************************************************
# ---FUNCTIONS----------------------------------------------------------------------------------------------------------
#source custom functions that are relevant to this module
# file.path(code.dir, '_lib', 'mod_fx.R') %>% source
##custom utilities##
#helper function to copy things out of R
writeExcel <- function(x,row.names=FALSE,col.names=TRUE,...) {
write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}
#label outliers statistically
isOutlier <- function(x) {
return(x < quantile(x, 0.25, na.rm=T) - 1.5 * IQR(x, na.rm=T) | x > quantile(x, 0.75, na.rm=T) + 1.5 * IQR(x, na.rm=T))
}
#***********************************************************************************************************************
# ---PREP DATA----------------------------------------------------------------------------------------------------------
##read in and prep datasets for analysis##
if(reload) {
#combine all the core datasets
#population dataset which will spread things out to be at IUs
pop_dt <- file.path(ihme_data_dir, pop_path) %>%
fread %>%
setnames(., names(.), names(.) %>% tolower) %>%
setnames(., c('ihme_loc_id', 'dens'), c('iso','pop_density')) %>%
.[, espen_loc := paste0(iso, '0', iu_id)]
#ntd mc dataset which we will filter the IUs by
iu_dt <- file.path(mc_data_dir) %>%
list.files(full.names = T) %>%
lapply(., fread, select=c('espen_loc')) %>%
rbindlist %>%
unique(by='espen_loc')
iu_list <- unique(iu_dt$espen_loc) %>%
str_remove_all(., "[^0-9.-]") %>% #remove isos
str_remove_all(., "^0+") %>% #remove trailing zeros
as.integer
#filter the pop dataset to the modelled IUs
#TODO, the pops file is not unique by IU...so we need to get the year variable added to this file but for now..
#i will just select the first row of each IU (latest year?)
#note we could also select the last row using (fromLast=T)
pop_dt <- pop_dt[iu_id%in%iu_list]%>%
unique(by='iu_id')
#lf dataset which is disease/scenario specific
lf_dt <- file.path(lf_data_dir, lf_path) %>%
fread %>%
setnames(., names(.), names(.) %>% tolower) %>%  #force names to lowercase for consistency
#impute the IMP decision constants
#TODO, validate these assumptions and eventually put them all into an assumption sheet to merge on?
.[, eco := 0] %>%
.[, vol := 0] %>%
.[, int := 1] %>%
.[, rds := 1] %>%  #TODO should be scenario specific but it's not present
.[, yrs := 5] %>%
.[, nat := 1] %>%  #TODO could invert subnat but its the same assumption
.[, sch := 1] %>%
.[, pop := str_replace_all(pop, ',', '') %>% as.numeric] %>% #convert from chr to numeric
.[, pop_treated := str_replace_all(`mda size`, ',', '') %>% as.numeric] %>% #convert from chr to numeric
.[, gdp := str_replace_all(gdp, ',', '') %>% as.numeric] %>% #convert from chr to numeric
#TODO not clear what to do with unit percentages...not given as decimals in table 2
.[, cov := str_replace(coverage, '%', '') %>% as.numeric] %>%
.[, cov := cov/1] #need to confirm this is the right way to unit the percentages
#there are 6 countries some missing pops/mda size/GDPs so i will impute them using world bank figures
lf_dt[iso=='BDI', gdp:= 221.48]
lf_dt[iso=='BFA', gdp:= 893.08]
lf_dt[iso=='MWI', gdp:= 634.84]
lf_dt[iso=='SLE', gdp:= 480.04]
lf_dt[iso=='SWZ', gdp:= 3978.4]
lf_dt[iso=='TZZ', gdp:= 1031] #zanzibar: i didnt see this split out on the world bank site so i used UN figures here:
#http://data.un.org/Data.aspx?d=SNAAMA&f=grID:101;currID:USD;pcFlag:1;crID:836
#add quintiles of gdp
lf_dt[, gdp_quint := cut(gdp, quantile(gdp, probs=0:5/5, na.rm=T), include.lowest=TRUE, labels=FALSE)]
}
# ----HEADER------------------------------------------------------------------------------------------------------------
# Author: JF
# Date: 01/18/2023
# Purpose: Prepare datasets and run delivery costs analysis
# source("/homes/jfrostad/_code/start/ipm/delivery_costs/calc.R", echo=T)
#***********************************************************************************************************************
# ----CONFIG------------------------------------------------------------------------------------------------------------
# clear memory
rm(list=ls())
#set opts
set.seed(98118)
options(scipen=999) #readability
#use cairo to render instead of quartz (quartz causes big slowdowns with geom_sf)
if(!identical(getOption("bitmapType"), "cairo") && isTRUE(capabilities()[["cairo"]])){
options(bitmapType = "cairo")
}
#set control flow params
reload <- T #set true if you want to reprep all the data
## Set core_repo locations
user            <- Sys.info()['user']
box_name        <- 'Joseph Frostad' #TODO find out how to pull this relative
local_dir       <- ifelse(Sys.info()["sysname"] == "Linux",
file.path('/homes', user, ''),
file.path('C:/Users', user, 'Documents/start/ipm/delivery_costs/'))
my_repo <- file.path(local_dir, 'repo')
my_dropbox <- file.path('C:/Users', user, 'UW START Dropbox', box_name, '/INTERNAL IPM Vx Valuations/NTD 2022')
setwd(my_repo)
#TODO categorize and comment packages
#TODO cleanup here
pacman::p_load(readxl, janitor, data.table, naniar, dplyr, stringr, magrittr, #data wrangling
scales, ggplot2, ggridges, ggrepel, gridExtra, RColorBrewer, viridis, #viz
stargazer, #sig tables
#caret, mlbench, randomForest, pls, #ML tools
zoo)
#***********************************************************************************************************************
# ----IN/OUT------------------------------------------------------------------------------------------------------------
###Input###
code_dir <- file.path(my_repo, 'code')
#TODO consider which data is universal and which is by disease. How to organize
model_data_dir <- file.path(my_dropbox, '/1 - Data gathering - all diseases/Costs/Delivery costs/')
coef_path <- "Delivery_cost_regression_coefficients_to_use_7_12_2022.csv" #TODO rename
#ihme data
ihme_version <- '2022-09-13'
ihme_data_dir <- file.path(my_dropbox, 'IHME', ihme_version)
pop_path <- 'IU_pop_dens_all_age.csv'
#ntd mc data
mc_data_dir <- file.path(my_dropbox, 'LF/NTD MC')
#TODO for now we are just doing this for LF. in the future should be a function with disease type as an argument
lf_version <- '2022-08-26'
lf_data_dir <- file.path(my_dropbox, paste0('LF/InputData ', lf_version))
lf_path <- 'MDA_costs_22_08_2022_AM.csv'
# example_path <- "Example data/Example_data_for_testing_code_7_12_2022.csv"
# cost_path <- 'Cost_data_23_08_2022_complete_to_share.csv'
###Output###
lf_out_dir <- file.path(my_dropbox, paste0('LF/OutputData ', lf_version))
out_dir <- file.path(local_dir, 'output')
viz_dir  <- file.path(local_dir, 'viz')
#***********************************************************************************************************************
# ---FUNCTIONS----------------------------------------------------------------------------------------------------------
#source custom functions that are relevant to this module
# file.path(code.dir, '_lib', 'mod_fx.R') %>% source
##custom utilities##
#helper function to copy things out of R
writeExcel <- function(x,row.names=FALSE,col.names=TRUE,...) {
write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}
#label outliers statistically
isOutlier <- function(x) {
return(x < quantile(x, 0.25, na.rm=T) - 1.5 * IQR(x, na.rm=T) | x > quantile(x, 0.75, na.rm=T) + 1.5 * IQR(x, na.rm=T))
}
#***********************************************************************************************************************
# ---PREP DATA----------------------------------------------------------------------------------------------------------
##read in and prep datasets for analysis##
if(reload) {
#combine all the core datasets
#population dataset which will spread things out to be at IUs
pop_dt <- file.path(ihme_data_dir, pop_path) %>%
fread %>%
setnames(., names(.), names(.) %>% tolower) %>%
setnames(., c('ihme_loc_id', 'dens'), c('iso','pop_density')) %>%
.[, espen_loc := paste0(iso, '0', iu_id)]
#ntd mc dataset which we will filter the IUs by
iu_dt <- file.path(mc_data_dir) %>%
list.files(full.names = T) %>%
lapply(., fread, select=c('espen_loc')) %>%
rbindlist %>%
unique(by='espen_loc')
iu_list <- unique(iu_dt$espen_loc) %>%
str_remove_all(., "[^0-9.-]") %>% #remove isos
str_remove_all(., "^0+") %>% #remove trailing zeros
as.integer
#filter the pop dataset to the modelled IUs
#TODO, the pops file is not unique by IU...so we need to get the year variable added to this file but for now..
#i will just select the first row of each IU (latest year?)
#note we could also select the last row using (fromLast=T)
pop_dt <- pop_dt[iu_id%in%iu_list]%>%
unique(by='iu_id')
#lf dataset which is disease/scenario specific
lf_dt <- file.path(lf_data_dir, lf_path) %>%
fread %>%
setnames(., names(.), names(.) %>% tolower) %>%  #force names to lowercase for consistency
#impute the IMP decision constants
#TODO, validate these assumptions and eventually put them all into an assumption sheet to merge on?
.[, eco := 0] %>%
.[, vol := 0] %>%
.[, int := 1] %>%
.[, rds := 1] %>%  #TODO should be scenario specific but it's not present
.[, yrs := 5] %>%
.[, nat := 1] %>%  #TODO could invert subnat but its the same assumption
.[, sch := 1] %>%
.[, pop := str_replace_all(pop, ',', '') %>% as.numeric] %>% #convert from chr to numeric
.[, pop_treated := str_replace_all(`mda size`, ',', '') %>% as.numeric] %>% #convert from chr to numeric
.[, gdp := str_replace_all(gdp, ',', '') %>% as.numeric] %>% #convert from chr to numeric
#TODO not clear what to do with unit percentages...not given as decimals in table 2
.[, cov := str_replace(coverage, '%', '') %>% as.numeric] %>%
.[, cov := cov/1] #need to confirm this is the right way to unit the percentages
#there are 6 countries some missing pops/mda size/GDPs so i will impute them using world bank figures
lf_dt[iso=='BDI', gdp:= 221.48]
lf_dt[iso=='BFA', gdp:= 893.08]
lf_dt[iso=='MWI', gdp:= 634.84]
lf_dt[iso=='SLE', gdp:= 480.04]
lf_dt[iso=='SWZ', gdp:= 3978.4]
lf_dt[iso=='TZZ', gdp:= 1031] #zanzibar: i didnt see this split out on the world bank site so i used UN figures here:
#http://data.un.org/Data.aspx?d=SNAAMA&f=grID:101;currID:USD;pcFlag:1;crID:836
#add quintiles of gdp
lf_dt[, gdp_quint := cut(gdp, quantile(gdp, probs=0:5/5, na.rm=T), include.lowest=TRUE, labels=FALSE)]
}
coef_dt <- file.path(model_data_dir, coef_path) %>%
fread %>%
setnames(., names(.), names(.) %>% tolower %>%  #force names to lowercase for consistency
str_replace(., 'model_', '') %>% #remove extraneous prefixes
str_replace(., 'log_', '') %>%
str_replace(., 'log', '') )
#rename the variables to match those from our cost dataset
setnames(coef_dt,
c('pop', 'den'),
c('pop_treated', 'pop_density'))
#reorder some things to match regression #1
irrelevant_coefs <- c('vut', 'kri', 'fri', 'mon', 'sqrtdis') #remove study dummies and the distance var from reg#2
coef_dt[, (irrelevant_coefs)  := NULL]
setcolorder(coef_dt,neworder='constant') #set constant first to match the model object format
#now use the extracted coefficients to predict using a custom function due to the complexity of the model.
#TODO we can do it like this for now but i think it's best to get the model object or the var:covar matrix from
#the authors, then predict like a regular lmer
#we could simulate the lmer if necessary using makeLmer but it requires more info about the model output
#here's a hacky way to make a fake lm with the provided coefficients
#first generate a fake dataset to run our lm over
fake_dt <- data.table(eco=rnorm(n = 1000, mean = 0, sd = 1),
vol=rnorm(n = 1000, mean = 0, sd = 1),
int=rnorm(n = 1000, mean = 0, sd = 1),
rds=rnorm(n = 1000, mean = 0, sd = 1),
yrs=rnorm(n = 1000, mean = 0, sd = 1),
cov=rnorm(n = 1000, mean = 0, sd = 1),
nat=rnorm(n = 1000, mean = 0, sd = 1),
sch=rnorm(n = 1000, mean = 0, sd = 1),
pop_treated=rnorm(n = 1000, mean = 0, sd = 1),
pop_density=rnorm(n = 1000, mean = 0, sd = 1),
gdp=rnorm(n = 1000, mean = 0, sd = 1),
cost=rnorm(n = 1000, mean = 0, sd = 1))
#setup a fake lm model that will store our extracted coefs
mod <- lm(cost~eco+vol+log(int)+log(rds)+
yrs+cov+nat+sch+log(pop_treated)+log(pop_density)+log(gdp)+
eco:log(int)+cov:nat+eco:sch+cov:sch+
nat:log(pop_treated)+nat:log(pop_density)+nat:log(gdp),
data=fake_dt)
#then inject our literature coefficients
#TODO setup some tests to assert that model formula matches coef dt order
coef_names <- mod$coefficients %>% names #extract the names for clarity because injecting erases them
#also make a vector of the ivs for later examination
coef_vars <- coef_names %>%
.[!(.%like%':')] %>%
.[!(.%like%'Intercept')] %>%
str_remove_all(., "log\\(") %>%
str_remove_all(., "\\)")
mod$coefficients <- unlist(coef_dt[1]) %>% as.numeric() #inject from the extraction dt
names(mod$coefficients) <- coef_names
#TODO test a few rows manually
lf_dt[, cost_natl := predict(mod, newdata =lf_dt) %>% exp]
lf_dt[, cost_natl := predict(mod, newdata =lf_dt) %>% exp]
#merge with pop density data to breakdown into IUs
setnames(lf_dt, c('pop_density'), c('pop_density_iso'))
lf_dt <- merge(lf_dt, pop_dt, by='iso', all.x=T, allow.cartesian = T)
#setnames(lf_dt, 'adj_pop', 'pop') #TODO confirm that this is the right value
#generate also the IU cost
#should we be changing nat to 0 now that these are subnational sites?
lf_dt[, nat := 0] #if we do it significantly drops the cost to more reasonable values
lf_dt[, cost_iu := predict(mod, newdata =lf_dt) %>% exp]
#output the results
write.csv(lf_dt, file=file.path(lf_out_dir, 'delivery_cost_preds.csv'))
lf_dt[, c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
#highlight outliers for plots
lf_dt[, iso_label := ifelse(isOutlier(cost_iu), iso, NA_character_)]
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=gdp_quint, label=iso_label)) +
geom_point(position='jitter') +
geom_text_repel() +
scale_color_viridis() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
file.path(viz_dir, 'natl_vs_iu_costs.png') %>% ggsave(height=8, width=12)
table(lf_dt$iso_label)
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=gdp_quint, text=iso_label)) +
geom_point(position='jitter') +
geom_text_repel() +
scale_color_viridis() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=gdp_quint, label=iso_label)) +
geom_point(position='jitter') +
geom_label_repel() +
scale_color_viridis() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
table(lf_dt$iso_label)
#highlight outliers for plots
lf_dt[, iso_label := ifelse(isOutlier(cost_iu), iso, NA_character_), by=gdp_quint]
table(lf_dt$iso_label)
lf_dt$iso_label <- NULL
#highlight outliers for plots
lf_dt[, iso_label := ifelse(isOutlier(cost_iu), iso, NA_character_), by=gdp_quint]
table(lf_dt$iso_label)
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=gdp_quint, label=iso_label)) +
geom_point(position='jitter') +
geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_viridis() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
file.path(viz_dir, 'natl_vs_iu_costs.png') %>% ggsave(height=8, width=12)
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label)) +
geom_point(position='jitter') +
geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer(palette='Set1') +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label)) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer(palette='Set1') +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
lf_dt$iso_label <- NULL
#highlight outliers for plots
lf_dt[, iso_label := ifelse(isOutlier(cost_iu), iso, NA_character_)]
lf_dt[is.na(iso_label), iso_label:="All Others"]
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label)) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer(palette='Set1') +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label)) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer(palette='Set1') +
scale_x_log10() +
scale_y_log10() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label)) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer('Outlier Countries', palette='Set1') +
scale_x_log10() +
scale_y_log10() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label)) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer('Outlier Countries', palette='Paired') +
scale_x_log10() +
scale_y_log10() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
file.path(viz_dir, 'natl_vs_iu_costs.png') %>% ggsave(height=8, width=12)
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label, size=log(gdp))) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer('Outlier Countries', palette='Paired') +
scale_x_log10() +
scale_y_log10() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
#scatter the IU preds against the natl preds to look for outliers
ggplot(lf_dt, aes(cost_iu, cost_natl, color=iso_label, size=gdp)) +
geom_point(position='jitter') +
#geom_text_repel(box.padding = 0.5, max.overlaps = Inf)  +
scale_color_brewer('Outlier Countries', palette='Paired') +
scale_x_log10() +
scale_y_log10() +
ggtitle("Delivery Cost Natl vs IU") +
theme_minimal()
file.path(viz_dir, 'natl_vs_iu_costs.png') %>% ggsave(height=8, width=12)
# ---DIAGNOSTICS--------------------------------------------------------------------------------------------------------
##examine some rows and test them against the online shiny tool
#this line can be used interactively to test different country and IU combinations in a compact manner
lf_dt[, c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
# ---DIAGNOSTICS--------------------------------------------------------------------------------------------------------
##examine some rows and test them against the online shiny tool
#this line can be used interactively to test different country and IU combinations in a compact manner
lf_dt[, c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
# ---DIAGNOSTICS--------------------------------------------------------------------------------------------------------
##examine some rows and test them against the online shiny tool
#this line can be used interactively to test different country and IU combinations in a compact manner
lf_dt[, c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
lf_dt[iso='NAM', c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
lf_dt[iso=='NAM', c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
lf_dt[iso=='NGA', c('iso', 'iu_id', coef_vars, 'pop_density_iso', 'cost_natl', 'cost_iu'), with=F]
summary(mod)
